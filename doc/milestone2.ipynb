{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation for SPLADTool\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "With the rapid development of deep learning, auto differentiation has become an indispensable part of multiple optimization algorithms like gradient descent. Numerical means such as Newton's Method and finite-difference method is useful in some situations, we desire to compute the analytical solutions by applying chain rules with our automatic differentiation SPLADTool (**S**imple **P**ytorch-**L**ike **A**uto **D**ifferentiation **Tool**kit), which will be faster and more accurate than numerical methods.\n",
    "\n",
    "## Background\n",
    "\n",
    "To help users better understand how automatic differentiation works, we will briefly explain some crucial background concepts applied in the calculations of automatic differentiation.\n",
    "\n",
    "### Jacobian Matrix\n",
    "\n",
    "Jacobian Matrix is an important concept in vector calculus, and it also helps us to understand the calculations in the automatic differentiation. Given a mapping $h: \\mathbb{R}^n \\to \\mathbb{R}^m$, the Jacobian matrix of h is as follows:\n",
    "\n",
    "$$\n",
    "J  =  {\\begin{bmatrix}{\\dfrac {\\partial h_1}{\\partial x_1}}&\\cdots &{\\dfrac {\\partial h_{1}}{\\partial x_{n}}}\\\\\\vdots &\\ddots &\\vdots \\\\{\\dfrac {\\partial h_{m}}{\\partial x_{1}}}&\\cdots &{\\dfrac {\\partial h_{m}}{\\partial x_{n}}}\\end{bmatrix}}\n",
    "$$\n",
    "\n",
    "### Chain Rule\n",
    "\n",
    "Chain rule is the most important concepts in Automatic Differentiation.\n",
    "\n",
    "Consider compound function $h(y(\\mathbf x))$, where $\\mathbf x \\in \\mathbb{R}^m$, where $y(\\mathbf x) = [y_1(\\mathbf x), y_2(\\mathbf x),\\cdots y_n(\\mathbf x)]^T$, The gradient of $h$ w.r.t $\\mathbf x$ can be computed as follows:\n",
    "\n",
    "$$\n",
    "\\nabla_\\mathbf x h = \\sum_{i=1}^n\\dfrac{\\partial h}{\\partial u}\\nabla_\\mathbf x u + \\dfrac{\\partial h}{\\partial v}\\nabla_\\mathbf x v\n",
    "$$\n",
    "\n",
    "### Gradient Computational Graph\n",
    "\n",
    "#### Forward Mode\n",
    "\n",
    "In forward mode, the gradients and the evaluations of basic nodes are computed all together along the forward process. We will use a simple function, $f(x) = \\sin(x) + x^2 + 1$ (evaluate at $x = 1$), as an example to illustrate the graph structure of calculations:\n",
    "\n",
    "We break down the function into several elementary functions. At each node, i.e. $x_i$, we only calculate one elementary operation, its corresponding derivative and their corresponding values. For the initial nodes, the gradients are assigned to be all $1$ vector.\n",
    "\n",
    "<img src=\"ForwardExample.png\">\n",
    "\n",
    "The corresponding trace table is:\n",
    "\n",
    "| trace | elementary operation | current value | elementary derivative | $\\nabla_x$  current value |\n",
    "| ----- | -------------------- | ------------- | --------------------- | ------------------------- |\n",
    "|$x_1$  | $x$          | $1$ | $\\dot{x}$          | $1$ |\n",
    "|$x_2$| $\\sin(x)$ | $\\sin(1)$ | $\\cos(x)\\dot{x}$| $\\cos(1)$|\n",
    "|$x_3$| $x^2$ | $1$ | $2x\\dot{x}$ | $2$ |\n",
    "|$x_4$| $x_2 + x_3$ | $\\sin(1) + 1$ | $\\cos(x)\\dot{x} + \\dot{x}$ | $\\cos(1) + 1$|\n",
    "|$x_5$| $x_4 + 1$ | $\\sin(1) + 2$ | $\\cos(x)\\dot{x} + \\dot{x}$ | $\\cos(1) + 2$|\n",
    "|$f$| $x_4 + 1$ | $\\sin(1) + 2$ | $\\cos(x)\\dot{x} + \\dot{x}$ | $\\cos(1) + 2$|\n",
    "\n",
    "#### Reverse Mode (Optional)\n",
    "\n",
    "In the reversed mode, the final node's gradient is set to 1. Along the forward process, only function evaluations will be done. Only during the backward process the gradients with respect to the nodes will be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Data Structures\n",
    "\n",
    "#### Tensor\n",
    "\n",
    "The core data structure here is the `spladtool.Tensor`, which contains the value vector (that will be represented by a `numpy.ndarray`) and corresponding gradient. \n",
    "\n",
    "In the reverse mode, we need two more attributes or member variables to keep record of the graph dependency: `Tensor.dependency` tracks the dependent tensor and `Tensor.layer` will store the layer or the operation used to attain this tensor. We will explain further how they are used. In the reverse mode, we also add a member function called `Tensor.backward()`, which will automatically call the `backward` method of `Tensor.layer` with arguments being `Tensor.dependency` to achieve reverse propagation.\n",
    "\n",
    "#### Layer\n",
    "\n",
    "A layer is defined as a basic operation, i.e. sum, product, division, sine function, etc.\n",
    "\n",
    "All layer classes inherit from a base class called `Layer`. For the forward mode, the member function `Layer.forward()` computes the evaluation and gradients altogether. In the reverse mode, `Layer.forward()` will only handle the evaluation, while `Layer.reverse()` will handle the gradients computation.\n",
    "\n",
    "### Functional APIs\n",
    "\n",
    "We wrapped up our implementations of operations in functional APIs which can be found in `spladtool_forward/functional.py` or `spladtool/functional.py`.\n",
    "\n",
    "We also add dunders or magic functions to `Tensor` class so that basic operators can be used on them.\n",
    "\n",
    "### Supported Operations(**New**)\n",
    "- Basic Operations: Add, Substract, Power, Negation, Product, Division\n",
    "- Analytical functions: trignomical, exponential, logarithm\n",
    "\n",
    "### Python Typing\n",
    "\n",
    "To make sure the type is correct, we add python typing to each of the operation classes and functional APIs to make sure the library will raise proper exceptions when encountered with unsupported operations.\n",
    "\n",
    "### Testing, CI & Coverage Report\n",
    "\n",
    "We adopt `unittest` as our testing framework. The up-to-now dev-only test script can be found in `./test.py`. As of continuous integration(CI), we are using Travis CI. For coverage report, we use the `coverage` package and upload the result to CodeCov. Find the results of our CI & coverage report by clicking on the badge in `README.md`\n",
    "\n",
    "### Future Extensions\n",
    "\n",
    "We have finished the forward and backward mode thus far, and provided a Newton-method-based equation solver and a simple regression optimizer as one of our test suite.  For future iterations of the code, we want to incorporate a `plot_comp_graph()` that will plot the forward mode of the computational graph and allow the user to visualize the gradients.  We also intend to include support for `jacobian`, `hessian`, and `Du` directional derivative operators for a function at a point.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Install dependencies(**updated**)\n",
    "\n",
    "    Instal our dependencies by `requirement.txt` with anaconda or python\n",
    "    ```bash\n",
    "    conda install --yes --file requirements.txt\n",
    "    # or\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "2. Try out an example from `test.py` on arithmetic functions:\n",
    "\n",
    "   ```python\n",
    "   import spladtool_forward as st\n",
    "   import splatool_forward.functional as F\n",
    "\n",
    "   x = st.tensor([[1., 2.], [3., 4.]])\n",
    "           \n",
    "   # Define output functions y(x) and z(x)\n",
    "   y = 2 * x + 1\n",
    "   z = - y / (x ** 3)\n",
    "   w = F.cos((F.exp(z) + F.exp(-z)) / 2)\n",
    "   \n",
    "   # Print out the values calculated by our forward mode automatic differentiation SPLADTool\n",
    "   print('x : ', x)\n",
    "   print('y : ', y)\n",
    "   print('y.grad : ', y.grad)\n",
    "   print('z: ', z)\n",
    "   print('z.grad: ', z.grad)\n",
    "   print('w: ', w)\n",
    "   print('w.grad: ', w.grad)\n",
    "   ```\n",
    "\n",
    "3. To run given tests, under UNIX environment, use\n",
    "```\n",
    "sh test.sh\n",
    "```\n",
    "\n",
    "Under Windows environment, run\n",
    "```\n",
    "coverage run test.py\n",
    "coverage report\n",
    "```\n",
    "\n",
    "## Software Organization\n",
    "\n",
    "```\n",
    "cs107-FinalProject/\n",
    "├── README.md\n",
    "├── LICENSE\n",
    "├── requirements.txt\n",
    "├── .travis.yml\n",
    "├── docs\n",
    "│   ├── documentation\n",
    "│   └── ...\n",
    "├── spladtool\n",
    "│   ├── __init__.py\n",
    "│   ├── functional.py\n",
    "│   ├── layer.py\n",
    "│   └── tensor.py\n",
    "├── tests\n",
    "│   ├── tests_basic.py\n",
    "│   ├── tests_comp.py\n",
    "│   └── tests_elem.py\n",
    "├── test.py\n",
    "└── test.sh\n",
    "```\n",
    "\n",
    "Our team plans to include the numpy module as the dependency of the auto-differentiation module and the torch, coverage, and codecov modules to perform tests. \n",
    "\n",
    "- The numpy package will be used to work with matrices and perform matrix operations\n",
    "- The torch package will be used to check our package against PyTorch's automatic differentiation engine, and the coverage and codecov modules will be used to produce coverage reports on the tests. (**This will only be needed in Dev mode**)\n",
    "\n",
    "The test suite will live in TravisCI and provide coverage reports to Codecov, where the reports are stored.\n",
    "\n",
    "In details, we need to do the following things:\n",
    "\n",
    "1. Add a licence to our software. See the **Licensing** section\n",
    "\n",
    "2. Create an conda virtual environment.\n",
    "\n",
    "3. Install all the dependencies.\n",
    "\n",
    "4. Register an account using an organization email\n",
    "\n",
    "5. Following PEP517, install `setuptools` ,`twine` and `build` by\n",
    "\n",
    "   ```shell\n",
    "   python3 -m pip install --upgrade setuptools build twine\n",
    "   ```\n",
    "\n",
    "6. Add a `setup.cfg` or `setup.py` configuration file.\n",
    "\n",
    "7. Execute build and upload by\n",
    "\n",
    "   ```shell\n",
    "   python3 -m build\n",
    "   python3 -m twine upload --repository testpypi dist/*\n",
    "   ```\n",
    "\n",
    "## Licensing\n",
    "\n",
    "This project will be licensed using the traditional MIT license due to several factors. \n",
    "\n",
    "- We will be using code from the NumPy library which the MIT license coincides with. \n",
    "- As of now, we do not foresee having to deal with any patents or any other dependencies. \n",
    "- Since this project won’t contain an abundance of novel code (and, therefore, could be duplicated quite easily), we don’t mind letting others use it as they please. \n",
    "- Due to the small scale of the project, we are hoping to use a license which is similarly simple. The MIT license is the best match for our interests outlined above. \n",
    "\n",
    "## Feedback\n",
    "\n",
    "### Milestone 1\n",
    "\n",
    "   1. Couldn't read the mathematical equations as they didn't render\n",
    "\t- Changed file to .ipynb for easier rendering\n",
    "    \n",
    "   2. Referred to reverse mode incorrectly as backward mode \n",
    "\t- Modified text to correct usage\n",
    "    \n",
    "   3. Didn't include how users should install/download package\n",
    "\t- Included more information about dependencies and included commands for users\n",
    "    \n",
    "   4. Didn't discuss packaging of software\n",
    "\t- Included basic process we will follow regarding building and uploading package"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
